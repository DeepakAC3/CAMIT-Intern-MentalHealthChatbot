{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLrk5CoGUZfr"
   },
   "source": [
    "**CREW AI IMPLEMENTATION WITH RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTlUdJxsKmRs"
   },
   "outputs": [],
   "source": [
    "# --- Install Required Packages ---\n",
    "!pip install -q langchain langchain-core langchain-experimental\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q langchain-community langchain-groq\n",
    "!pip install -q crewai spacy textblob gradio chromadb sentence-transformers pydantic\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import shutil\n",
    "import spacy\n",
    "import tempfile\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from typing import Type\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Global Variables ---\n",
    "VECTOR_DB = None\n",
    "NLP_MODEL = None\n",
    "CHATBOT_INSTANCE = None  # Store single chatbot instance\n",
    "\n",
    "# --- Utility: Clean ChromaDB Directory ---\n",
    "def clean_chromadb_dir():\n",
    "    # Use a unique temporary directory with timestamp to avoid conflicts\n",
    "    timestamp = str(int(time.time()))\n",
    "    chroma_db_path = os.path.join(tempfile.gettempdir(), f'chroma_db_{timestamp}')\n",
    "\n",
    "    # Also clean any old chroma_db directories\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    for item in os.listdir(temp_dir):\n",
    "        if item.startswith('chroma_db'):\n",
    "            old_path = os.path.join(temp_dir, item)\n",
    "            if os.path.isdir(old_path):\n",
    "                try:\n",
    "                    shutil.rmtree(old_path)\n",
    "                    print(f'Deleted old ChromaDB directory: {old_path}')\n",
    "                except:\n",
    "                    pass  # Ignore if can't delete (might be in use)\n",
    "\n",
    "    print(f'Using ChromaDB directory: {chroma_db_path}')\n",
    "    return chroma_db_path\n",
    "\n",
    "# --- NLP Initialization ---\n",
    "def initialize_nlp():\n",
    "    global NLP_MODEL\n",
    "    if NLP_MODEL is None:\n",
    "        try:\n",
    "            NLP_MODEL = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            os.system(\"python -m spacy download en_core_web_sm\")\n",
    "            NLP_MODEL = spacy.load(\"en_core_web_sm\")\n",
    "    return NLP_MODEL\n",
    "\n",
    "# --- Vector DB Loader/Creator ---\n",
    "def fetch_pdfs_from_repo():\n",
    "    repo_url = \"https://github.com/DeepakAC3/CAMIT-Intern-MentalHealthChatbot.git\"\n",
    "    repo_folder = \"CAMIT-Intern-MentalHealthChatbot\"\n",
    "    data_folder = \"./data/\"\n",
    "\n",
    "    # Only clone if data folder doesn't exist or is empty\n",
    "    if not os.path.exists(data_folder) or not os.listdir(data_folder):\n",
    "        # Clone repo if not already cloned\n",
    "        if not os.path.exists(repo_folder):\n",
    "            print(f\"Cloning repo {repo_url}...\")\n",
    "            subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "        else:\n",
    "            print(f\"Repo folder '{repo_folder}' already exists. Skipping clone.\")\n",
    "\n",
    "        # Copy PDF files from repo data folder to local data folder\n",
    "        os.makedirs(data_folder, exist_ok=True)\n",
    "        src_data_folder = os.path.join(repo_folder, \"data\")\n",
    "        if os.path.exists(src_data_folder):\n",
    "            print(f\"Copying PDFs from {src_data_folder} to {data_folder}...\")\n",
    "            for filename in os.listdir(src_data_folder):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    src_file = os.path.join(src_data_folder, filename)\n",
    "                    dst_file = os.path.join(data_folder, filename)\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "            print(\"PDF files copied successfully.\")\n",
    "        else:\n",
    "            print(f\"No data folder found in repo at {src_data_folder}\")\n",
    "\n",
    "        # Delete the cloned repo folder to save space\n",
    "        if os.path.exists(repo_folder):\n",
    "            print(f\"Removing cloned repo folder '{repo_folder}'...\")\n",
    "            shutil.rmtree(repo_folder)\n",
    "    else:\n",
    "        print(\"PDF files already exist in ./data/, skipping download.\")\n",
    "\n",
    "def load_existing_vector_db():\n",
    "    global VECTOR_DB\n",
    "    db_path = clean_chromadb_dir()  # Get a clean, writable directory\n",
    "    print(\"Creating new ChromaDB in writable location...\")\n",
    "    return create_vector_db_from_data(db_path)\n",
    "\n",
    "def create_vector_db_from_data(db_path):\n",
    "    global VECTOR_DB\n",
    "    os.makedirs('./data/', exist_ok=True)\n",
    "\n",
    "    # Ensure the ChromaDB directory exists and is writable\n",
    "    os.makedirs(db_path, exist_ok=True)\n",
    "\n",
    "    loader = DirectoryLoader(\"./data/\", glob='*.pdf', loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No PDF documents found in ./data/ directory. Creating empty vector store...\")\n",
    "        VECTOR_DB = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "        return VECTOR_DB\n",
    "\n",
    "    print(f\"Found {len(documents)} PDF documents. Processing...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} text chunks\")\n",
    "\n",
    "    # Create vector database with explicit collection name to avoid conflicts\n",
    "    VECTOR_DB = Chroma.from_documents(\n",
    "        texts,\n",
    "        embeddings,\n",
    "        persist_directory=db_path,\n",
    "        collection_name=f\"mental_health_docs_{int(time.time())}\"  # Unique collection name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        VECTOR_DB.persist()\n",
    "        print(\"Database persisted successfully\")\n",
    "    except AttributeError:\n",
    "        print(\"Auto-persistence enabled\")\n",
    "\n",
    "    print(f\"ChromaDB created with {len(texts)} document chunks and saved to {db_path}\")\n",
    "    return VECTOR_DB\n",
    "\n",
    "# --- CrewAI Tools ---\n",
    "class KnowledgeRetrievalInput(BaseModel):\n",
    "    query: str = Field(description=\"The query to search for in the knowledge base\")\n",
    "\n",
    "class KnowledgeRetrievalTool(BaseTool):\n",
    "    name: str = \"knowledge_retrieval\"\n",
    "    description: str = \"Retrieves relevant mental health information from the knowledge base\"\n",
    "    args_schema: Type[BaseModel] = KnowledgeRetrievalInput\n",
    "    def _run(self, query: str) -> str:\n",
    "        global VECTOR_DB\n",
    "        try:\n",
    "            if VECTOR_DB is None:\n",
    "                return \"Knowledge base not initialized\"\n",
    "            retriever = VECTOR_DB.as_retriever(search_kwargs={\"k\": 3})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            return f\"Retrieved context: {context}\" if context else \"No relevant context found\"\n",
    "        except Exception as e:\n",
    "            return f\"Error retrieving knowledge: {str(e)}\"\n",
    "\n",
    "class EmotionAnalysisInput(BaseModel):\n",
    "    text: str = Field(description=\"The text to analyze for emotional content\")\n",
    "\n",
    "class EmotionAnalysisTool(BaseTool):\n",
    "    name: str = \"emotion_analysis\"\n",
    "    description: str = \"Analyzes emotional state and sentiment from user input\"\n",
    "    args_schema: Type[BaseModel] = EmotionAnalysisInput\n",
    "    def _run(self, text: str) -> str:\n",
    "        try:\n",
    "            nlp = initialize_nlp()\n",
    "            doc = nlp(text)\n",
    "            blob = TextBlob(text)\n",
    "            lemmatized_tokens = [\n",
    "                token.lemma_.lower()\n",
    "                for token in doc\n",
    "                if not token.is_stop and not token.is_punct and len(token.text) > 2\n",
    "            ]\n",
    "            emotion_lexicon = {\n",
    "                'anxiety': ['anxious', 'worried', 'nervous', 'stressed', 'panic'],\n",
    "                'depression': ['sad', 'depressed', 'hopeless', 'empty', 'worthless'],\n",
    "                'anger': ['angry', 'frustrated', 'irritated', 'mad', 'furious'],\n",
    "                'joy': ['happy', 'excited', 'joyful', 'content', 'pleased']\n",
    "            }\n",
    "            detected_emotions = []\n",
    "            for emotion, words in emotion_lexicon.items():\n",
    "                for token in lemmatized_tokens:\n",
    "                    if token in words:\n",
    "                        detected_emotions.append(emotion)\n",
    "            analysis = {\n",
    "                'sentiment_polarity': blob.sentiment.polarity,\n",
    "                'sentiment_subjectivity': blob.sentiment.subjectivity,\n",
    "                'detected_emotions': list(set(detected_emotions)),\n",
    "                'emotional_intensity': 'high' if abs(blob.sentiment.polarity) > 0.5 else 'moderate' if abs(blob.sentiment.polarity) > 0.2 else 'low'\n",
    "            }\n",
    "            return f\"Emotional analysis: {analysis}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error analyzing emotion: {str(e)}\"\n",
    "\n",
    "class SafetyMonitorInput(BaseModel):\n",
    "    text: str = Field(description=\"The text to monitor for safety concerns\")\n",
    "\n",
    "class SafetyMonitorTool(BaseTool):\n",
    "    name: str = \"safety_monitor\"\n",
    "    description: str = \"Monitors for crisis indicators and safety concerns\"\n",
    "    args_schema: Type[BaseModel] = SafetyMonitorInput\n",
    "    def _run(self, text: str) -> str:\n",
    "        crisis_keywords = [\n",
    "            'suicide', 'kill myself', 'end it all', 'hurt myself', 'want to die',\n",
    "            'self harm', 'cut myself', 'overdose', 'jump off', 'hang myself'\n",
    "        ]\n",
    "        urgent_keywords = [\n",
    "            'emergency', 'crisis', 'help me', 'cant take it', 'breaking point'\n",
    "        ]\n",
    "        text_lower = text.lower()\n",
    "        crisis_detected = any(keyword in text_lower for keyword in crisis_keywords)\n",
    "        urgent_detected = any(keyword in text_lower for keyword in urgent_keywords)\n",
    "        if crisis_detected:\n",
    "            return \"CRISIS_ALERT: Immediate suicide risk detected. Emergency intervention required.\"\n",
    "        elif urgent_detected:\n",
    "            return \"URGENT_CONCERN: High distress level detected. Immediate support recommended.\"\n",
    "        else:\n",
    "            return \"SAFE: No immediate safety concerns detected.\"\n",
    "\n",
    "# --- CrewAI Chatbot Class ---\n",
    "class CrewAIMentalHealthChatbot:\n",
    "    def __init__(self):\n",
    "        self.setup_llm()\n",
    "        self.setup_vector_db()\n",
    "        self.setup_tools()\n",
    "        self.setup_agents()\n",
    "\n",
    "    def setup_llm(self):\n",
    "        self.llm = LLM(\n",
    "            model=\"groq/llama-3.3-70b-versatile\",\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\", \"YOUR_GROK_API_KEY\"),\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "    def setup_vector_db(self):\n",
    "        global VECTOR_DB\n",
    "        fetch_pdfs_from_repo()\n",
    "        VECTOR_DB = load_existing_vector_db()\n",
    "        if VECTOR_DB is not None:\n",
    "            try:\n",
    "                test_docs = VECTOR_DB.get()\n",
    "                doc_count = len(test_docs['documents'])\n",
    "                print(f\"Vector database ready with {doc_count} documents\")\n",
    "                if doc_count == 0:\n",
    "                    print(\"Note: Vector database is empty. Add PDF files to ./data/ folder and restart.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Vector database may not be functioning properly: {str(e)}\")\n",
    "\n",
    "    def setup_tools(self):\n",
    "        initialize_nlp()\n",
    "        self.knowledge_tool = KnowledgeRetrievalTool()\n",
    "        self.emotion_tool = EmotionAnalysisTool()\n",
    "        self.safety_tool = SafetyMonitorTool()\n",
    "\n",
    "    def setup_agents(self):\n",
    "        self.emotion_agent = Agent(\n",
    "            role='Emotional Intelligence Specialist',\n",
    "            goal='Analyze user emotions and psychological state accurately',\n",
    "            backstory=\"\"\"You are an expert in emotional intelligence and psychological assessment.\n",
    "            Your role is to carefully analyze user messages for emotional content, sentiment,\n",
    "            and psychological indicators. You provide detailed emotional assessments that help\n",
    "            other agents understand the user's mental state.\"\"\",\n",
    "            tools=[self.emotion_tool],\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            allow_delegation=False\n",
    "        )\n",
    "        self.knowledge_agent = Agent(\n",
    "            role='Mental Health Knowledge Specialist',\n",
    "            goal='Retrieve and synthesize relevant mental health information',\n",
    "            backstory=\"\"\"You are a mental health knowledge specialist with access to\n",
    "            comprehensive mental health resources. Your role is to find and synthesize\n",
    "            relevant information from the knowledge base to support therapeutic conversations.\"\"\",\n",
    "            tools=[self.knowledge_tool],\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            allow_delegation=False\n",
    "        )\n",
    "        self.safety_agent = Agent(\n",
    "            role='Crisis Intervention Specialist',\n",
    "            goal='Monitor for safety risks and crisis situations',\n",
    "            backstory=\"\"\"You are a crisis intervention specialist responsible for identifying\n",
    "            immediate safety risks and crisis situations. Your primary concern is user safety,\n",
    "            and you must flag any concerning content that requires immediate attention.\"\"\",\n",
    "            tools=[self.safety_tool],\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            allow_delegation=False\n",
    "        )\n",
    "        self.therapist_agent = Agent(\n",
    "            role='Compassionate Mental Health Therapist',\n",
    "            goal='Provide empathetic, therapeutic responses based on analysis from other agents',\n",
    "            backstory=\"\"\"You are a compassionate mental health therapist with expertise in\n",
    "            various therapeutic approaches. You synthesize information from emotional analysis,\n",
    "            knowledge retrieval, and safety assessments to provide thoughtful, empathetic,\n",
    "            and therapeutically appropriate responses to users.\"\"\",\n",
    "            tools=[],\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            allow_delegation=False\n",
    "        )\n",
    "\n",
    "    def chat(self, user_input: str) -> str:\n",
    "        try:\n",
    "            emotion_task = Task(\n",
    "                description=f\"Use the emotion_analysis tool to analyze this message: '{user_input}'. Provide detailed emotional insights.\",\n",
    "                agent=self.emotion_agent,\n",
    "                expected_output=\"Detailed emotional analysis including sentiment, detected emotions, and intensity level\"\n",
    "            )\n",
    "            knowledge_task = Task(\n",
    "                description=f\"Use the knowledge_retrieval tool to find relevant mental health information for: '{user_input}'. Focus on therapeutic guidance.\",\n",
    "                agent=self.knowledge_agent,\n",
    "                expected_output=\"Relevant mental health information and therapeutic guidance from knowledge base\"\n",
    "            )\n",
    "            safety_task = Task(\n",
    "                description=f\"Use the safety_monitor tool to assess this message for crisis indicators: '{user_input}'. Report any safety concerns.\",\n",
    "                agent=self.safety_agent,\n",
    "                expected_output=\"Safety assessment with risk level and any crisis alerts\"\n",
    "            )\n",
    "            therapy_task = Task(\n",
    "                description=f\"\"\"Based on the previous analyses, provide a compassionate therapeutic response to: '{user_input}'.\n",
    "                Consider the emotional state, relevant knowledge, and safety concerns to craft an appropriate response.\n",
    "                Be warm, empathetic, and provide helpful guidance while validating the user's feelings.\"\"\",\n",
    "                agent=self.therapist_agent,\n",
    "                expected_output=\"Compassionate therapeutic response addressing the user's needs\",\n",
    "                context=[emotion_task, knowledge_task, safety_task]\n",
    "            )\n",
    "            task_crew = Crew(\n",
    "                agents=[self.emotion_agent, self.knowledge_agent, self.safety_agent, self.therapist_agent],\n",
    "                tasks=[emotion_task, knowledge_task, safety_task, therapy_task],\n",
    "                process=Process.sequential,\n",
    "                verbose=False\n",
    "            )\n",
    "            result = task_crew.kickoff()\n",
    "            if hasattr(result, 'raw'):\n",
    "                return result.raw\n",
    "            else:\n",
    "                return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I encountered an issue processing your message. Please try again. Error: {str(e)}\"\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "def create_gradio_interface():\n",
    "    global CHATBOT_INSTANCE\n",
    "\n",
    "    # Reuse existing chatbot instance instead of creating a new one\n",
    "    if CHATBOT_INSTANCE is None:\n",
    "        print(\"Initializing CrewAI Mental Health Chatbot for Gradio...\")\n",
    "        CHATBOT_INSTANCE = CrewAIMentalHealthChatbot()\n",
    "    else:\n",
    "        print(\"Using existing chatbot instance for Gradio...\")\n",
    "\n",
    "    def respond(user_message, chat_history):\n",
    "        bot_response = CHATBOT_INSTANCE.chat(user_message)\n",
    "        return bot_response\n",
    "\n",
    "    demo = gr.ChatInterface(\n",
    "        fn=respond,\n",
    "        title=\"🧠 CrewAI Multi-Agent Mental Health Chatbot\",\n",
    "        description=\"\"\"\n",
    "        An advanced mental health chatbot powered by CrewAI multi-agent workflow:\n",
    "        • **Emotion Agent**: Analyzes your emotional state\n",
    "        • **Knowledge Agent**: Retrieves relevant mental health information\n",
    "        • **Safety Agent**: Monitors for crisis situations\n",
    "        • **Therapist Agent**: Provides compassionate therapeutic responses\n",
    "        \"\"\",\n",
    "        examples=[\n",
    "            \"I'm feeling overwhelmed with work stress\",\n",
    "            \"I've been having trouble sleeping and feeling anxious\",\n",
    "            \"Can you help me understand depression better?\",\n",
    "            \"I'm going through a difficult breakup\"\n",
    "        ],\n",
    "        theme=\"soft\"\n",
    "    )\n",
    "    return demo\n",
    "\n",
    "# --- Command Line Interface ---\n",
    "def main():\n",
    "    global CHATBOT_INSTANCE\n",
    "\n",
    "    print(\"Initializing CrewAI Multi-Agent Mental Health Chatbot...\")\n",
    "    CHATBOT_INSTANCE = CrewAIMentalHealthChatbot()\n",
    "\n",
    "    print(\"\\n🧠 CrewAI Mental Health Chatbot Ready!\")\n",
    "    print(\"Type 'exit' to quit, 'gradio' to launch web interface\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Take care! Remember, professional help is always available if you need it.\")\n",
    "            break\n",
    "        elif user_input.lower() == 'gradio':\n",
    "            print(\"Launching Gradio interface...\")\n",
    "            demo = create_gradio_interface()\n",
    "            demo.launch()\n",
    "            break\n",
    "\n",
    "        print(\"🤖 Processing with multi-agent workflow...\")\n",
    "        response = CHATBOT_INSTANCE.chat(user_input)\n",
    "        print(f\"Chatbot: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set environment variable for Groq API key\n",
    "    os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROK_API_KEY\"\n",
    "    # Download spaCy model (if needed)\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5sYqjnzUhbv"
   },
   "source": [
    "**TELEGRAM BOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E9leXUSndJ"
   },
   "outputs": [],
   "source": [
    "!pip install nest-asyncio python-telegram-bot\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "\n",
    "\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\"Hello! I'm your mental health assistant. How can I help you today?\")\n",
    "\n",
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_message = update.message.text\n",
    "    response = CrewAIMentalHealthChatbot.chat(user_message)\n",
    "    await update.message.reply_text(response)\n",
    "\n",
    "def main():\n",
    "    app = ApplicationBuilder().token(\"YOUR_TELEGRAM_BOT_TOKEN\").build()\n",
    "    app.add_handler(CommandHandler(\"start\", start))\n",
    "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
    "    app.run_polling()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDq49E0TIAd9"
   },
   "source": [
    "**METRICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIO3t1--GpiQ",
    "outputId": "b1838c1c-3dcd-41ab-d1b4-e9ca4aba04bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "/tmp/ipython-input-25-346116600.py:64: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory()\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing performance analysis...\n",
      "Starting comprehensive performance analysis...\n",
      "============================================================\n",
      "Calculating Cosine Similarity Metrics...\n",
      "Calculating Task Completion Metrics...\n",
      "Calculating Inter-Agent Latency...\n",
      "Calculating Memory Usage...\n",
      "Calculating Parallelism Efficiency...\n",
      "Calculating Throughput...\n",
      "Calculating Accuracy and Quality Score...\n",
      "Calculating False Positive Rate...\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "MENTAL HEALTH CHATBOT PERFORMANCE ANALYSIS REPORT\n",
      "================================================\n",
      "\n",
      "1. COSINE SIMILARITY METRICS\n",
      "----------------------------\n",
      "Average Similarity: 0.3402\n",
      "Max Similarity: 0.4281\n",
      "Min Similarity: 0.1878\n",
      "\n",
      "2. TASK COMPLETION METRICS\n",
      "--------------------------\n",
      "Success Rate: 100.00%\n",
      "Average Completion Time: 1023.71 ms\n",
      "Total Tasks: 5\n",
      "Successful Tasks: 5\n",
      "\n",
      "3. INTER-AGENT LATENCY\n",
      "----------------------\n",
      "Average Retrieval Latency: 10.50 ms\n",
      "Average Emotion Analysis Latency: 8.00 ms\n",
      "Average Safety Check Latency: 0.00 ms\n",
      "Average Response Generation Latency: 1280.98 ms\n",
      "Average Total Pipeline Latency: 1299.48 ms\n",
      "\n",
      "4. MEMORY USAGE\n",
      "---------------\n",
      "Baseline Memory: 2370.43 MB\n",
      "Average Memory Usage: 2370.43 MB\n",
      "Peak Memory Usage: 2370.43 MB\n",
      "Average Memory Increase: 0.00 MB\n",
      "\n",
      "5. PARALLELISM EFFICIENCY\n",
      "-------------------------\n",
      "Sequential Time: 4.90 seconds\n",
      "Parallel Time: 2.25 seconds\n",
      "Speedup Factor: 2.18x\n",
      "Efficiency: 72.54%\n",
      "\n",
      "6. THROUGHPUT ANALYSIS\n",
      "----------------------\n",
      "Batch Size 1: 0.86 queries/second\n",
      "Batch Size 3: 0.93 queries/second\n",
      "Batch Size 5: 0.43 queries/second\n",
      "\n",
      "7. ACCURACY & QUALITY SCORE\n",
      "----------------------------\n",
      "Overall Quality Score: 0.8825\n",
      "Context Relevance: 0.6789\n",
      "Response Coherence: 1.0000\n",
      "Emotional Appropriateness: 0.8511\n",
      "Safety Compliance: 1.0000\n",
      "\n",
      "8. FALSE POSITIVE RATE ANALYSIS\n",
      "-------------------------------\n",
      "False Positive Rate: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "9. ORCHESTRATION EFFECTIVENESS SUMMARY\n",
      "--------------------------------------\n",
      "Task Success Rate: 100.00%\n",
      "Average Response Time: 1299.48 ms\n",
      "System Reliability: 100.00%\n",
      "Resource Efficiency: 72.54%\n",
      "        \n",
      "\n",
      "Detailed metrics saved to 'performance_metrics.json'\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "class PerformanceMetricsCalculator:\n",
    "    def __init__(self):\n",
    "        self.setup_components()\n",
    "        self.metrics = {}\n",
    "        self.test_queries = [\n",
    "            \"I'm feeling anxious about my exam\",\n",
    "            \"I've been having trouble sleeping\",\n",
    "            \"Can you help me with stress management?\",\n",
    "            \"I feel overwhelmed with work\",\n",
    "            \"I'm experiencing panic attacks\"\n",
    "        ]\n",
    "\n",
    "    def setup_components(self):\n",
    "        \"\"\"Initialize all system components\"\"\"\n",
    "        # LLM setup\n",
    "        self.llm = ChatGroq(\n",
    "            temperature=0,\n",
    "            groq_api_key=\"YOUR_GROK_API_KEY\",\n",
    "            model_name=\"llama-3.3-70b-versatile\"\n",
    "        )\n",
    "\n",
    "        # Embeddings and vector store\n",
    "        self.embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "        )\n",
    "        # Load or create vector database using temporary directory (same as your working chatbot)\n",
    "\n",
    "        db_path = '/tmp/chroma_db_1751459108'\n",
    "        if os.path.exists(db_path):\n",
    "            self.vector_db = Chroma(persist_directory=db_path, embedding_function=self.embeddings)\n",
    "        else:\n",
    "            # Create empty vector store for testing\n",
    "            self.vector_db = Chroma(persist_directory=db_path, embedding_function=self.embeddings)\n",
    "\n",
    "        # NLP components\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            os.system(\"python -m spacy download en_core_web_sm\")\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        # Memory\n",
    "        self.memory = ConversationBufferMemory()\n",
    "\n",
    "        # Sentence transformer for similarity calculations\n",
    "        self.sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    def convert_to_serializable(self, obj):\n",
    "        \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self.convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def calculate_cosine_similarity_metrics(self):\n",
    "        \"\"\"Calculate cosine similarity effectiveness\"\"\"\n",
    "        print(\"Calculating Cosine Similarity Metrics...\")\n",
    "\n",
    "        # Create test embeddings\n",
    "        test_embeddings = self.sentence_model.encode(self.test_queries)\n",
    "\n",
    "        # Calculate pairwise similarities\n",
    "        similarity_matrix = cosine_similarity(test_embeddings)\n",
    "\n",
    "        # Calculate average similarity scores\n",
    "        avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "        max_similarity = np.max(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "        min_similarity = np.min(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "\n",
    "        self.metrics['cosine_similarity'] = {\n",
    "            'average_similarity': float(avg_similarity),\n",
    "            'max_similarity': float(max_similarity),\n",
    "            'min_similarity': float(min_similarity),\n",
    "            'similarity_matrix': similarity_matrix.tolist()\n",
    "        }\n",
    "\n",
    "        return self.metrics['cosine_similarity']\n",
    "\n",
    "    def calculate_task_completion_metrics(self):\n",
    "        \"\"\"Calculate task completion time and success rate\"\"\"\n",
    "        print(\"Calculating Task Completion Metrics...\")\n",
    "\n",
    "        completion_times = []\n",
    "        success_count = 0\n",
    "        total_tasks = len(self.test_queries)\n",
    "\n",
    "        for query in self.test_queries:\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Simulate full pipeline\n",
    "                response = self.process_query_pipeline(query)\n",
    "                end_time = time.time()\n",
    "\n",
    "                completion_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "                completion_times.append(completion_time)\n",
    "\n",
    "                if response and len(response) > 10:  # Basic success criteria\n",
    "                    success_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed for query '{query}': {e}\")\n",
    "                completion_times.append(float('inf'))\n",
    "\n",
    "        success_rate = (success_count / total_tasks) * 100\n",
    "        avg_completion_time = np.mean([t for t in completion_times if t != float('inf')])\n",
    "\n",
    "        self.metrics['task_completion'] = {\n",
    "            'success_rate': float(success_rate),\n",
    "            'average_completion_time_ms': float(avg_completion_time),\n",
    "            'completion_times': [float(t) if t != float('inf') else None for t in completion_times],\n",
    "            'total_tasks': int(total_tasks),\n",
    "            'successful_tasks': int(success_count)\n",
    "        }\n",
    "\n",
    "        return self.metrics['task_completion']\n",
    "\n",
    "    def calculate_inter_agent_latency(self):\n",
    "        \"\"\"Calculate latency between different agent components\"\"\"\n",
    "        print(\"Calculating Inter-Agent Latency...\")\n",
    "\n",
    "        latencies = {\n",
    "            'retrieval_latency': [],\n",
    "            'emotion_analysis_latency': [],\n",
    "            'safety_check_latency': [],\n",
    "            'response_generation_latency': [],\n",
    "            'total_pipeline_latency': []\n",
    "        }\n",
    "\n",
    "        for query in self.test_queries:\n",
    "            # Measure retrieval agent latency\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "                docs = retriever.get_relevant_documents(query)\n",
    "                retrieval_time = (time.time() - start_time) * 1000\n",
    "                latencies['retrieval_latency'].append(retrieval_time)\n",
    "            except:\n",
    "                latencies['retrieval_latency'].append(0)\n",
    "\n",
    "            # Measure emotion analysis latency\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                doc = self.nlp(query)\n",
    "                blob = TextBlob(query)\n",
    "                sentiment = blob.sentiment\n",
    "                emotion_time = (time.time() - start_time) * 1000\n",
    "                latencies['emotion_analysis_latency'].append(emotion_time)\n",
    "            except:\n",
    "                latencies['emotion_analysis_latency'].append(0)\n",
    "\n",
    "            # Measure safety check latency\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                crisis_keywords = ['suicide', 'kill myself', 'hurt myself']\n",
    "                safety_check = any(keyword in query.lower() for keyword in crisis_keywords)\n",
    "                safety_time = (time.time() - start_time) * 1000\n",
    "                latencies['safety_check_latency'].append(safety_time)\n",
    "            except:\n",
    "                latencies['safety_check_latency'].append(0)\n",
    "\n",
    "            # Measure response generation latency\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                response = self.llm.invoke(f\"Respond to: {query}\")\n",
    "                response_time = (time.time() - start_time) * 1000\n",
    "                latencies['response_generation_latency'].append(response_time)\n",
    "            except:\n",
    "                latencies['response_generation_latency'].append(0)\n",
    "\n",
    "            # Calculate total pipeline latency\n",
    "            total_latency = sum([\n",
    "                latencies['retrieval_latency'][-1],\n",
    "                latencies['emotion_analysis_latency'][-1],\n",
    "                latencies['safety_check_latency'][-1],\n",
    "                latencies['response_generation_latency'][-1]\n",
    "            ])\n",
    "            latencies['total_pipeline_latency'].append(total_latency)\n",
    "\n",
    "        # Calculate averages and convert to serializable format\n",
    "        avg_latencies = {}\n",
    "        for key, values in latencies.items():\n",
    "            avg_latencies[f'avg_{key}'] = float(np.mean(values))\n",
    "            avg_latencies[f'max_{key}'] = float(np.max(values))\n",
    "            avg_latencies[f'min_{key}'] = float(np.min(values))\n",
    "\n",
    "        self.metrics['inter_agent_latency'] = {\n",
    "            'detailed_latencies': {k: [float(v) for v in values] for k, values in latencies.items()},\n",
    "            'average_latencies': avg_latencies\n",
    "        }\n",
    "\n",
    "        return self.metrics['inter_agent_latency']\n",
    "\n",
    "    def calculate_memory_usage(self):\n",
    "        \"\"\"Calculate memory usage and computational efficiency\"\"\"\n",
    "        print(\"Calculating Memory Usage...\")\n",
    "\n",
    "        process = psutil.Process()\n",
    "\n",
    "        # Baseline memory\n",
    "        baseline_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "        # Memory during operations\n",
    "        memory_readings = []\n",
    "\n",
    "        for query in self.test_queries:\n",
    "            # Memory before processing\n",
    "            mem_before = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "            # Process query\n",
    "            try:\n",
    "                self.process_query_pipeline(query)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Memory after processing\n",
    "            mem_after = process.memory_info().rss / 1024 / 1024\n",
    "            memory_readings.append({\n",
    "                'before': float(mem_before),\n",
    "                'after': float(mem_after),\n",
    "                'difference': float(mem_after - mem_before)\n",
    "            })\n",
    "\n",
    "        # Calculate memory statistics\n",
    "        avg_memory_usage = float(np.mean([r['after'] for r in memory_readings]))\n",
    "        peak_memory_usage = float(np.max([r['after'] for r in memory_readings]))\n",
    "        avg_memory_increase = float(np.mean([r['difference'] for r in memory_readings]))\n",
    "\n",
    "        self.metrics['memory_usage'] = {\n",
    "            'baseline_memory_mb': float(baseline_memory),\n",
    "            'average_memory_mb': avg_memory_usage,\n",
    "            'peak_memory_mb': peak_memory_usage,\n",
    "            'average_memory_increase_mb': avg_memory_increase,\n",
    "            'detailed_readings': memory_readings\n",
    "        }\n",
    "\n",
    "        return self.metrics['memory_usage']\n",
    "\n",
    "    def calculate_parallelism_efficiency(self):\n",
    "        \"\"\"Calculate parallelism efficiency vs sequential processing\"\"\"\n",
    "        print(\"Calculating Parallelism Efficiency...\")\n",
    "\n",
    "        # Sequential processing time\n",
    "        start_time = time.time()\n",
    "        sequential_results = []\n",
    "        for query in self.test_queries:\n",
    "            try:\n",
    "                result = self.process_query_pipeline(query)\n",
    "                sequential_results.append(result)\n",
    "            except:\n",
    "                sequential_results.append(None)\n",
    "        sequential_time = time.time() - start_time\n",
    "\n",
    "        # Parallel processing time\n",
    "        start_time = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            parallel_results = list(executor.map(self.process_query_pipeline, self.test_queries))\n",
    "        parallel_time = time.time() - start_time\n",
    "\n",
    "        # Calculate efficiency\n",
    "        speedup = sequential_time / parallel_time if parallel_time > 0 else 0\n",
    "        efficiency = speedup / 3  # 3 workers\n",
    "\n",
    "        self.metrics['parallelism_efficiency'] = {\n",
    "            'sequential_time_seconds': float(sequential_time),\n",
    "            'parallel_time_seconds': float(parallel_time),\n",
    "            'speedup_factor': float(speedup),\n",
    "            'efficiency_percentage': float(efficiency * 100),\n",
    "            'theoretical_max_speedup': 3\n",
    "        }\n",
    "\n",
    "        return self.metrics['parallelism_efficiency']\n",
    "\n",
    "    def calculate_throughput(self):\n",
    "        \"\"\"Calculate system throughput\"\"\"\n",
    "        print(\"Calculating Throughput...\")\n",
    "\n",
    "        # Test different batch sizes\n",
    "        batch_sizes = [1, 3, 5, 10]\n",
    "        throughput_results = {}\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            test_batch = self.test_queries[:min(batch_size, len(self.test_queries))]\n",
    "\n",
    "            start_time = time.time()\n",
    "            processed_count = 0\n",
    "\n",
    "            for query in test_batch:\n",
    "                try:\n",
    "                    self.process_query_pipeline(query)\n",
    "                    processed_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            throughput = processed_count / duration if duration > 0 else 0\n",
    "\n",
    "            throughput_results[f'batch_size_{batch_size}'] = {\n",
    "                'queries_per_second': float(throughput),\n",
    "                'total_queries': int(len(test_batch)),\n",
    "                'successful_queries': int(processed_count),\n",
    "                'duration_seconds': float(duration)\n",
    "            }\n",
    "\n",
    "        self.metrics['throughput'] = throughput_results\n",
    "        return self.metrics['throughput']\n",
    "\n",
    "    def calculate_accuracy_quality_score(self):\n",
    "        \"\"\"Calculate accuracy and quality metrics\"\"\"\n",
    "        print(\"Calculating Accuracy and Quality Score...\")\n",
    "\n",
    "        quality_scores = []\n",
    "        accuracy_metrics = {\n",
    "            'context_relevance': [],\n",
    "            'response_coherence': [],\n",
    "            'emotional_appropriateness': [],\n",
    "            'safety_compliance': []\n",
    "        }\n",
    "\n",
    "        for query in self.test_queries:\n",
    "            try:\n",
    "                response = self.process_query_pipeline(query)\n",
    "\n",
    "                # Context relevance (cosine similarity between query and response)\n",
    "                query_embedding = self.sentence_model.encode([query])\n",
    "                response_embedding = self.sentence_model.encode([response])\n",
    "                relevance_score = float(cosine_similarity(query_embedding, response_embedding)[0][0])\n",
    "                accuracy_metrics['context_relevance'].append(relevance_score)\n",
    "\n",
    "                # Response coherence (length and structure)\n",
    "                coherence_score = min(len(response.split()) / 50, 1.0)  # Normalize by expected length\n",
    "                accuracy_metrics['response_coherence'].append(float(coherence_score))\n",
    "\n",
    "                # Emotional appropriateness (sentiment alignment)\n",
    "                query_sentiment = TextBlob(query).sentiment.polarity\n",
    "                response_sentiment = TextBlob(response).sentiment.polarity\n",
    "                emotion_score = 1 - abs(query_sentiment - response_sentiment) / 2\n",
    "                accuracy_metrics['emotional_appropriateness'].append(float(emotion_score))\n",
    "\n",
    "                # Safety compliance (no harmful content)\n",
    "                harmful_keywords = ['harmful', 'dangerous', 'suicide', 'self-harm']\n",
    "                safety_score = 1.0 if not any(word in response.lower() for word in harmful_keywords) else 0.5\n",
    "                accuracy_metrics['safety_compliance'].append(float(safety_score))\n",
    "\n",
    "                # Overall quality score\n",
    "                overall_quality = np.mean([\n",
    "                    relevance_score,\n",
    "                    coherence_score,\n",
    "                    emotion_score,\n",
    "                    safety_score\n",
    "                ])\n",
    "                quality_scores.append(float(overall_quality))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating quality for query '{query}': {e}\")\n",
    "                quality_scores.append(0.0)\n",
    "                for key in accuracy_metrics:\n",
    "                    accuracy_metrics[key].append(0.0)\n",
    "\n",
    "        # Calculate averages\n",
    "        avg_quality_score = float(np.mean(quality_scores))\n",
    "        avg_accuracy_metrics = {key: float(np.mean(values)) for key, values in accuracy_metrics.items()}\n",
    "\n",
    "        self.metrics['accuracy_quality'] = {\n",
    "            'overall_quality_score': avg_quality_score,\n",
    "            'individual_quality_scores': quality_scores,\n",
    "            'accuracy_breakdown': avg_accuracy_metrics,\n",
    "            'detailed_accuracy_metrics': {k: [float(v) for v in values] for k, values in accuracy_metrics.items()}\n",
    "        }\n",
    "\n",
    "        return self.metrics['accuracy_quality']\n",
    "\n",
    "    def calculate_false_positive_rate(self):\n",
    "        \"\"\"Calculate false positive rate for crisis detection\"\"\"\n",
    "        print(\"Calculating False Positive Rate...\")\n",
    "\n",
    "        # Test queries with known crisis/non-crisis labels\n",
    "        crisis_test_cases = [\n",
    "            (\"I want to hurt myself\", True),\n",
    "            (\"I'm feeling sad today\", False),\n",
    "            (\"I can't take it anymore, I want to end it all\", True),\n",
    "            (\"I'm stressed about work\", False),\n",
    "            (\"I'm having suicidal thoughts\", True),\n",
    "            (\"I feel overwhelmed but I'll manage\", False)\n",
    "        ]\n",
    "\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        for query, is_crisis in crisis_test_cases:\n",
    "            detected_crisis = self.detect_crisis(query)\n",
    "\n",
    "            if is_crisis and detected_crisis:\n",
    "                true_positives += 1\n",
    "            elif is_crisis and not detected_crisis:\n",
    "                false_negatives += 1\n",
    "            elif not is_crisis and detected_crisis:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "\n",
    "        # Calculate rates\n",
    "        total_negative = true_negatives + false_positives\n",
    "        false_positive_rate = false_positives / total_negative if total_negative > 0 else 0\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        self.metrics['false_positive_rate'] = {\n",
    "            'false_positive_rate': float(false_positive_rate),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1_score': float(f1_score),\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(true_positives),\n",
    "                'false_positives': int(false_positives),\n",
    "                'true_negatives': int(true_negatives),\n",
    "                'false_negatives': int(false_negatives)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return self.metrics['false_positive_rate']\n",
    "\n",
    "    def detect_crisis(self, text):\n",
    "        \"\"\"Helper method to detect crisis indicators\"\"\"\n",
    "        crisis_keywords = ['suicide', 'kill myself', 'end it all', 'hurt myself', 'want to die', 'suicidal']\n",
    "        return any(keyword in text.lower() for keyword in crisis_keywords)\n",
    "\n",
    "    def process_query_pipeline(self, query):\n",
    "        \"\"\"Simulate the full query processing pipeline\"\"\"\n",
    "        try:\n",
    "            # Retrieval\n",
    "            retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs]) if docs else \"No context available\"\n",
    "\n",
    "            # Emotion analysis\n",
    "            doc = self.nlp(query)\n",
    "            blob = TextBlob(query)\n",
    "            sentiment = blob.sentiment\n",
    "\n",
    "            # Safety check\n",
    "            crisis_detected = self.detect_crisis(query)\n",
    "\n",
    "            if crisis_detected:\n",
    "                return \"I understand you're going through a difficult time. Please reach out to a mental health professional or crisis hotline immediately.\"\n",
    "\n",
    "            # Generate response\n",
    "            prompt = f\"Context: {context}\\nUser query: {query}\\nProvide a compassionate response:\"\n",
    "            response = self.llm.invoke(prompt)\n",
    "\n",
    "            return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {str(e)}\"\n",
    "\n",
    "    def run_all_calculations(self):\n",
    "        \"\"\"Run all performance metric calculations\"\"\"\n",
    "        print(\"Starting comprehensive performance analysis...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Run all calculations\n",
    "        self.calculate_cosine_similarity_metrics()\n",
    "        self.calculate_task_completion_metrics()\n",
    "        self.calculate_inter_agent_latency()\n",
    "        self.calculate_memory_usage()\n",
    "        self.calculate_parallelism_efficiency()\n",
    "        self.calculate_throughput()\n",
    "        self.calculate_accuracy_quality_score()\n",
    "        self.calculate_false_positive_rate()\n",
    "\n",
    "        # Convert all metrics to serializable format\n",
    "        self.metrics = self.convert_to_serializable(self.metrics)\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a comprehensive performance report\"\"\"\n",
    "        if not self.metrics:\n",
    "            self.run_all_calculations()\n",
    "\n",
    "        report = \"\"\"\n",
    "MENTAL HEALTH CHATBOT PERFORMANCE ANALYSIS REPORT\n",
    "================================================\n",
    "\n",
    "1. COSINE SIMILARITY METRICS\n",
    "----------------------------\n",
    "Average Similarity: {:.4f}\n",
    "Max Similarity: {:.4f}\n",
    "Min Similarity: {:.4f}\n",
    "\n",
    "2. TASK COMPLETION METRICS\n",
    "--------------------------\n",
    "Success Rate: {:.2f}%\n",
    "Average Completion Time: {:.2f} ms\n",
    "Total Tasks: {}\n",
    "Successful Tasks: {}\n",
    "\n",
    "3. INTER-AGENT LATENCY\n",
    "----------------------\n",
    "Average Retrieval Latency: {:.2f} ms\n",
    "Average Emotion Analysis Latency: {:.2f} ms\n",
    "Average Safety Check Latency: {:.2f} ms\n",
    "Average Response Generation Latency: {:.2f} ms\n",
    "Average Total Pipeline Latency: {:.2f} ms\n",
    "\n",
    "4. MEMORY USAGE\n",
    "---------------\n",
    "Baseline Memory: {:.2f} MB\n",
    "Average Memory Usage: {:.2f} MB\n",
    "Peak Memory Usage: {:.2f} MB\n",
    "Average Memory Increase: {:.2f} MB\n",
    "\n",
    "5. PARALLELISM EFFICIENCY\n",
    "-------------------------\n",
    "Sequential Time: {:.2f} seconds\n",
    "Parallel Time: {:.2f} seconds\n",
    "Speedup Factor: {:.2f}x\n",
    "Efficiency: {:.2f}%\n",
    "\n",
    "6. THROUGHPUT ANALYSIS\n",
    "----------------------\n",
    "Batch Size 1: {:.2f} queries/second\n",
    "Batch Size 3: {:.2f} queries/second\n",
    "Batch Size 5: {:.2f} queries/second\n",
    "\n",
    "7. ACCURACY & QUALITY SCORE\n",
    "----------------------------\n",
    "Overall Quality Score: {:.4f}\n",
    "Context Relevance: {:.4f}\n",
    "Response Coherence: {:.4f}\n",
    "Emotional Appropriateness: {:.4f}\n",
    "Safety Compliance: {:.4f}\n",
    "\n",
    "8. FALSE POSITIVE RATE ANALYSIS\n",
    "-------------------------------\n",
    "False Positive Rate: {:.4f}\n",
    "Precision: {:.4f}\n",
    "Recall: {:.4f}\n",
    "F1 Score: {:.4f}\n",
    "\n",
    "9. ORCHESTRATION EFFECTIVENESS SUMMARY\n",
    "--------------------------------------\n",
    "Task Success Rate: {:.2f}%\n",
    "Average Response Time: {:.2f} ms\n",
    "System Reliability: {:.2f}%\n",
    "Resource Efficiency: {:.2f}%\n",
    "        \"\"\".format(\n",
    "            self.metrics['cosine_similarity']['average_similarity'],\n",
    "            self.metrics['cosine_similarity']['max_similarity'],\n",
    "            self.metrics['cosine_similarity']['min_similarity'],\n",
    "\n",
    "            self.metrics['task_completion']['success_rate'],\n",
    "            self.metrics['task_completion']['average_completion_time_ms'],\n",
    "            self.metrics['task_completion']['total_tasks'],\n",
    "            self.metrics['task_completion']['successful_tasks'],\n",
    "\n",
    "            self.metrics['inter_agent_latency']['average_latencies']['avg_retrieval_latency'],\n",
    "            self.metrics['inter_agent_latency']['average_latencies']['avg_emotion_analysis_latency'],\n",
    "            self.metrics['inter_agent_latency']['average_latencies']['avg_safety_check_latency'],\n",
    "            self.metrics['inter_agent_latency']['average_latencies']['avg_response_generation_latency'],\n",
    "            self.metrics['inter_agent_latency']['average_latencies']['avg_total_pipeline_latency'],\n",
    "\n",
    "            self.metrics['memory_usage']['baseline_memory_mb'],\n",
    "            self.metrics['memory_usage']['average_memory_mb'],\n",
    "            self.metrics['memory_usage']['peak_memory_mb'],\n",
    "            self.metrics['memory_usage']['average_memory_increase_mb'],\n",
    "\n",
    "            self.metrics['parallelism_efficiency']['sequential_time_seconds'],\n",
    "            self.metrics['parallelism_efficiency']['parallel_time_seconds'],\n",
    "            self.metrics['parallelism_efficiency']['speedup_factor'],\n",
    "            self.metrics['parallelism_efficiency']['efficiency_percentage'],\n",
    "\n",
    "            self.metrics['throughput']['batch_size_1']['queries_per_second'],\n",
    "            self.metrics['throughput']['batch_size_3']['queries_per_second'],\n",
    "            self.metrics['throughput']['batch_size_5']['queries_per_second'],\n",
    "\n",
    "            self.metrics['accuracy_quality']['overall_quality_score'],\n",
    "            self.metrics['accuracy_quality']['accuracy_breakdown']['context_relevance'],\n",
    "            self.metrics['accuracy_quality']['accuracy_breakdown']['response_coherence'],\n",
    "            self.metrics['accuracy_quality']['accuracy_breakdown']['emotional_appropriateness'],\n",
    "            self.metrics['accuracy_quality']['accuracy_breakdown']['safety_compliance'],\n",
    "\n",
    "            self.metrics['false_positive_rate']['false_positive_rate'],\n",
    "            self.metrics['false_positive_rate']['precision'],\n",
    "            self.metrics['false_positive_rate']['recall'],\n",
    "            self.metrics['false_positive_rate']['f1_score'],\n",
    "\n",
    "            self.metrics['task_completion']['success_rate'],\n",
    "            self.metrics['inter_agent_latency']['average_latencies']['avg_total_pipeline_latency'],\n",
    "            (1 - self.metrics['false_positive_rate']['false_positive_rate']) * 100,\n",
    "            self.metrics['parallelism_efficiency']['efficiency_percentage']\n",
    "        )\n",
    "\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create calculator instance\n",
    "    calculator = PerformanceMetricsCalculator()\n",
    "\n",
    "    # Run all calculations\n",
    "    print(\"Initializing performance analysis...\")\n",
    "    metrics = calculator.run_all_calculations()\n",
    "\n",
    "    # Generate and print report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    report = calculator.generate_report()\n",
    "    print(report)\n",
    "\n",
    "    # Save detailed metrics to file (now with proper JSON serialization)\n",
    "    try:\n",
    "        with open('performance_metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(\"\\nDetailed metrics saved to 'performance_metrics.json'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving metrics: {e}\")\n",
    "\n",
    "    print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
