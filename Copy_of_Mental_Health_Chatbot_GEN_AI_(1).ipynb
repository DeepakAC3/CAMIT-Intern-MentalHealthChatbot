{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installs** the three LangChain packages (langchain_groq, langchain_core, langchain_community) so the notebook can call Groq’s LLM and use LangChain utilities."
      ],
      "metadata": {
        "id": "IzA7hl0ZG6Oa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQNGtHdU6OJS",
        "outputId": "39412ab3-3f41-4382-da96-815ea30f628c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.66)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: groq<1,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.29.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain_core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.28.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.28.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_groq langchain_core langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Imports ChatGroq from langchain_groq.\n",
        "\n",
        "* Creates an LLM instance with your API key and the “llama-3.3-70b-versatile” model.\n"
      ],
      "metadata": {
        "id": "VxjJzS-FHPFZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXHOpaH46qNz",
        "outputId": "dfec55c3-e853-4d9d-c25a-a6db5f507519",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello and welcome. I'm so glad you've taken this first step in reaching out for support. My purpose is to provide a safe, non-judgmental space for you to express yourself and explore your feelings. I'm here to listen and offer guidance, not to diagnose or prescribe, but to help you navigate through any challenges you might be facing.\n",
            "\n",
            "Please know that everything discussed here is confidential and respected. You're in control of the conversation, and we can go at your pace. If you need time to think or don't feel like talking about something, that's perfectly okay. We can change subjects or simply be present together in silence if that's what you need.\n",
            "\n",
            "How have you been feeling lately? Is there something specific on your mind that you'd like to talk about, or perhaps you're not sure where to start? I'm here to listen and support you in any way I can.\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(\n",
        "    temperature = 0,\n",
        "    groq_api_key = \"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\",\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")\n",
        "result = llm.invoke(\"You are a mental health chatbot, try to provide assistance for the user and make them feel comfortable.\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installs pypdf; this lightweight library lets later cells read and parse PDF files that will be fed into the vector database.**"
      ],
      "metadata": {
        "id": "G0tL-y0rHlRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EMrh3XG8oET",
        "outputId": "4f43b1cc-b102-469f-daaa-794eb283472b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installs chromadb plus its many dependencies; Chroma is the local vector store used to hold chunk embeddings so the chatbot can retrieve contextual passages at run-time.**"
      ],
      "metadata": {
        "id": "u0N8-tphHoIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC_k5J25_OPC",
        "outputId": "1535759e-f3aa-47ec-988d-961fd19c7e14",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSTghfuaAMSW",
        "outputId": "86bbc604-0b09-4cd0-8a6e-7c13fb05adbc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports everything the rest of the notebook needs**"
      ],
      "metadata": {
        "id": "73OT0ZNZHzXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-l2n0dE7Jj7"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wraps the ChatGroq constructor in a helper so you can spin up the same LLM configuration anywhere with one call.**"
      ],
      "metadata": {
        "id": "R5K5z4VyH6w5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_BQRVGRJkEh"
      },
      "outputs": [],
      "source": [
        "def initialize_llm():\n",
        "  llm = ChatGroq(\n",
        "    temperature = 0,\n",
        "    groq_api_key = \"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\",\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")\n",
        "  return llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loads every PDF inside /content/data/.**\n",
        "\n",
        "**Splits each document into 500-character chunks with 50-char overlap.**\n",
        "\n",
        "**Embeds the chunks with all-MiniLM-L6-v2.**\n",
        "\n",
        "**Stores the resulting vectors in a persistent Chroma database at ./chroma_db and tells you when it’s done.**"
      ],
      "metadata": {
        "id": "vQ6Jb6_nIAMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_db():\n",
        "  loader = DirectoryLoader(\"/content/data/\", glob = '*.pdf', loader_cls = PyPDFLoader)\n",
        "  documents = loader.load()\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')\n",
        "  vector_db.persist()\n",
        "\n",
        "  print(\"ChromaDB created and data saved\")\n",
        "\n",
        "  return vector_db\n",
        "\n"
      ],
      "metadata": {
        "id": "nn-rqdjnE7-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes a Chroma retriever and LLM, then builds a LangChain RetrievalQA pipeline that:\n",
        "– Inserts retrieved context plus the user’s question into a custom prompt (“You are a compassionate mental health chatbot…”)\n",
        "– Lets the LLM generate the final answer."
      ],
      "metadata": {
        "id": "tsWmY2kvIakR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_qa_chain(vector_db, llm):\n",
        "  retriever = vector_db.as_retriever()\n",
        "  prompt_templates = \"\"\" You are a compassionate mental health chatbot. Respond thoughtfully to the following question:\n",
        "    {context}\n",
        "    User: {question}\n",
        "    Chatbot: \"\"\"\n",
        "  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "      llm = llm,\n",
        "      chain_type = \"stuff\",\n",
        "      retriever = retriever,\n",
        "      chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "  )\n",
        "  return qa_chain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AJbG2LADE-n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks whether a Chroma database already exists; if not, it builds one from the PDFs with create_vector_db(); otherwise it reloads the stored vectors.\n",
        "\n",
        "Builds the Retrieval-QA pipeline by passing the vector store and LLM to setup_qa_chain().\n",
        "\n",
        "Enters an infinite chat loop: waits for user text, breaks cleanly on exit, or pipes any other query through qa_chain.run() and prints the chatbot’s response"
      ],
      "metadata": {
        "id": "YgC7JzQAJ4xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.makedirs('/content/data/', exist_ok=True)\n",
        "# create_vector_db()"
      ],
      "metadata": {
        "id": "qYtGn0gl9sGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  print(\"Intializing Chatbot.........\")\n",
        "  llm = initialize_llm()\n",
        "\n",
        "  db_path = \"/content/chroma_db\"\n",
        "\n",
        "  if not os.path.exists(db_path):\n",
        "    vector_db  = create_vector_db()\n",
        "  else:\n",
        "    embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "  qa_chain = setup_qa_chain(vector_db, llm)\n",
        "\n",
        "  while True:\n",
        "    query = input(\"\\nHuman: \")\n",
        "    if query.lower()  == \"exit\":\n",
        "      print(\"Chatbot: Take Care of yourself, Goodbye!\")\n",
        "      break\n",
        "    response = qa_chain.run(query)\n",
        "    print(f\"Chatbot: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oIQjjLuFBih",
        "outputId": "55bef3aa-1dc9-48f7-8681-996ce7f95c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intializing Chatbot.........\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Human: exit\n",
            "Chatbot: Take Care of yourself, Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gradio\n"
      ],
      "metadata": {
        "id": "nPisNaT4LaZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "llm = initialize_llm()\n",
        "db_path = \"/content/chroma_db\"\n",
        "\n",
        "if not os.path.exists(db_path):\n",
        "    vector_db = create_vector_db()\n",
        "else:\n",
        "    embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "\n",
        "qa_chain = setup_qa_chain(vector_db, llm)\n",
        "\n",
        "def answer(user_message, chat_history):\n",
        "    return qa_chain.run(user_message)\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer,\n",
        "    title=\"🧠 Mental-Health PDF Chatbot\",\n",
        "    description=\"Ask anything about the uploaded PDFs — type *exit* in the CLI or just close the tab to quit.\",\n",
        ")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "9ndmxZJuLbRw",
        "outputId": "1e133c52-949b-4c7f-8f77-5f78c2004495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://863e06bc1a39231ab0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://863e06bc1a39231ab0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy textblob langchain-experimental\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "DXf_bxuuHzW2",
        "outputId": "0e41a949-8c78-4368-ae4a-e046b45b8d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.26)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.66)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.8)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.23.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-experimental\n",
            "Successfully installed langchain-experimental-0.3.4\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.tools import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import spacy\n",
        "\n",
        "class MultiAgentMentalHealthChatbot:\n",
        "    def __init__(self):\n",
        "        self.llm = self.initialize_llm()\n",
        "        self.vector_db = self.load_vector_db()\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.memory = ConversationBufferMemory()\n",
        "\n",
        "        # Initialize specialized agents\n",
        "        self.retrieval_agent = self.create_retrieval_agent()\n",
        "        self.emotion_agent = self.create_emotion_agent()\n",
        "        self.response_agent = self.create_response_agent()\n",
        "        self.safety_agent = self.create_safety_agent()\n",
        "\n",
        "    def create_retrieval_agent(self):\n",
        "        def retrieve_context(query: str) -> str:\n",
        "            retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "            docs = retriever.get_relevant_documents(query)\n",
        "            return \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        return Tool(\n",
        "            name=\"knowledge_retrieval\",\n",
        "            description=\"Retrieves relevant mental health information from knowledge base\",\n",
        "            func=retrieve_context\n",
        "        )\n",
        "\n",
        "    def create_emotion_agent(self):\n",
        "        def analyze_emotion(text: str) -> dict:\n",
        "            doc = self.nlp(text)\n",
        "\n",
        "            # Extract emotional indicators\n",
        "            emotions = {\n",
        "                'sentiment': self.get_sentiment_score(doc),\n",
        "                'emotional_words': [token.lemma_ for token in doc if token.pos_ in ['ADJ', 'VERB'] and token.sentiment != 0],\n",
        "                'intensity': self.calculate_emotional_intensity(doc),\n",
        "                'crisis_indicators': self.detect_crisis_words(doc)\n",
        "            }\n",
        "            return emotions\n",
        "\n",
        "        return Tool(\n",
        "            name=\"emotion_analysis\",\n",
        "            description=\"Analyzes emotional state and sentiment from user input\",\n",
        "            func=lambda x: str(analyze_emotion(x))\n",
        "        )\n"
      ],
      "metadata": {
        "id": "3F5Qgr4LIE-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from textblob import TextBlob\n",
        "\n",
        "class EmotionProcessor:\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            # Download model if not available\n",
        "            spacy.cli.download(\"en_core_web_sm\")\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Add sentiment component\n",
        "        if \"textcat\" not in self.nlp.pipe_names:\n",
        "            self.nlp.add_pipe(\"textcat\")\n",
        "\n",
        "    def process_text(self, text: str) -> dict:\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Lemmatization with filtering\n",
        "        lemmatized_tokens = [\n",
        "            token.lemma_.lower()\n",
        "            for token in doc\n",
        "            if not token.is_stop\n",
        "            and not token.is_punct\n",
        "            and not token.is_space\n",
        "            and len(token.text) > 2\n",
        "        ]\n",
        "\n",
        "        # Emotional analysis\n",
        "        emotional_analysis = {\n",
        "            'lemmatized_text': ' '.join(lemmatized_tokens),\n",
        "            'entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
        "            'sentiment': self.get_sentiment(text),\n",
        "            'emotional_keywords': self.extract_emotional_keywords(doc),\n",
        "            'pos_tags': [(token.text, token.pos_) for token in doc]\n",
        "        }\n",
        "\n",
        "        return emotional_analysis\n",
        "\n",
        "    def get_sentiment(self, text: str) -> dict:\n",
        "        blob = TextBlob(text)\n",
        "        return {\n",
        "            'polarity': blob.sentiment.polarity,\n",
        "            'subjectivity': blob.sentiment.subjectivity,\n",
        "            'classification': self.classify_sentiment(blob.sentiment.polarity)\n",
        "        }\n",
        "\n",
        "    def classify_sentiment(self, polarity: float) -> str:\n",
        "        if polarity > 0.1:\n",
        "            return \"positive\"\n",
        "        elif polarity < -0.1:\n",
        "            return \"negative\"\n",
        "        else:\n",
        "            return \"neutral\"\n",
        "\n",
        "    def extract_emotional_keywords(self, doc) -> list:\n",
        "        emotional_words = []\n",
        "        emotion_lexicon = {\n",
        "            'anxiety': ['anxious', 'worried', 'nervous', 'stressed'],\n",
        "            'depression': ['sad', 'depressed', 'hopeless', 'empty'],\n",
        "            'anger': ['angry', 'frustrated', 'irritated', 'mad'],\n",
        "            'joy': ['happy', 'excited', 'joyful', 'content']\n",
        "        }\n",
        "\n",
        "        for token in doc:\n",
        "            lemma = token.lemma_.lower()\n",
        "            for emotion, words in emotion_lexicon.items():\n",
        "                if lemma in words:\n",
        "                    emotional_words.append((lemma, emotion))\n",
        "\n",
        "        return emotional_words\n"
      ],
      "metadata": {
        "id": "MRx7kCWzJdf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import gradio as gr\n",
        "\n",
        "class EmotionProcessor:\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            # Download model if not available\n",
        "            os.system(\"python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def process_text(self, text: str) -> dict:\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Lemmatization with filtering\n",
        "        lemmatized_tokens = [\n",
        "            token.lemma_.lower()\n",
        "            for token in doc\n",
        "            if not token.is_stop\n",
        "            and not token.is_punct\n",
        "            and not token.is_space\n",
        "            and len(token.text) > 2\n",
        "        ]\n",
        "\n",
        "        # Emotional analysis\n",
        "        emotional_analysis = {\n",
        "            'lemmatized_text': ' '.join(lemmatized_tokens),\n",
        "            'entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
        "            'sentiment': self.get_sentiment(text),\n",
        "            'emotional_keywords': self.extract_emotional_keywords(doc),\n",
        "            'pos_tags': [(token.text, token.pos_) for token in doc]\n",
        "        }\n",
        "\n",
        "        return emotional_analysis\n",
        "\n",
        "    def get_sentiment(self, text: str) -> dict:\n",
        "        blob = TextBlob(text)\n",
        "        return {\n",
        "            'polarity': blob.sentiment.polarity,\n",
        "            'subjectivity': blob.sentiment.subjectivity,\n",
        "            'classification': self.classify_sentiment(blob.sentiment.polarity)\n",
        "        }\n",
        "\n",
        "    def classify_sentiment(self, polarity: float) -> str:\n",
        "        if polarity > 0.1:\n",
        "            return \"positive\"\n",
        "        elif polarity < -0.1:\n",
        "            return \"negative\"\n",
        "        else:\n",
        "            return \"neutral\"\n",
        "\n",
        "    def extract_emotional_keywords(self, doc) -> list:\n",
        "        emotional_words = []\n",
        "        emotion_lexicon = {\n",
        "            'anxiety': ['anxious', 'worried', 'nervous', 'stressed'],\n",
        "            'depression': ['sad', 'depressed', 'hopeless', 'empty'],\n",
        "            'anger': ['angry', 'frustrated', 'irritated', 'mad'],\n",
        "            'joy': ['happy', 'excited', 'joyful', 'content']\n",
        "        }\n",
        "\n",
        "        for token in doc:\n",
        "            lemma = token.lemma_.lower()\n",
        "            for emotion, words in emotion_lexicon.items():\n",
        "                if lemma in words:\n",
        "                    emotional_words.append((lemma, emotion))\n",
        "\n",
        "        return emotional_words\n",
        "\n",
        "class MentalHealthMultiAgent:\n",
        "    def __init__(self):\n",
        "        self.setup_components()\n",
        "        self.create_tools()\n",
        "        self.setup_qa_chain()\n",
        "\n",
        "    def setup_components(self):\n",
        "        # Initialize LLM\n",
        "        self.llm = ChatGroq(\n",
        "            temperature=0,\n",
        "            groq_api_key=\"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\",\n",
        "            model_name=\"llama-3.3-70b-versatile\"\n",
        "        )\n",
        "\n",
        "        # Load or create vector database\n",
        "        self.vector_db = self.load_vector_db()\n",
        "\n",
        "        # Initialize emotion processor\n",
        "        self.emotion_processor = EmotionProcessor()\n",
        "\n",
        "        # Initialize memory\n",
        "        self.conversation_memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "    def load_vector_db(self):\n",
        "        db_path = \"./chroma_db\"\n",
        "\n",
        "        if not os.path.exists(db_path):\n",
        "            return self.create_vector_db()\n",
        "        else:\n",
        "            embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "            return Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "\n",
        "    def create_vector_db(self):\n",
        "        # Create data directory if it doesn't exist\n",
        "        os.makedirs('./data/', exist_ok=True)\n",
        "\n",
        "        loader = DirectoryLoader(\"./data/\", glob='*.pdf', loader_cls=PyPDFLoader)\n",
        "        documents = loader.load()\n",
        "\n",
        "        if not documents:\n",
        "            print(\"No PDF documents found in ./data/ directory\")\n",
        "            # Create empty vector store\n",
        "            embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "            vector_db = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)\n",
        "            return vector_db\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "        vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')\n",
        "        vector_db.persist()\n",
        "\n",
        "        print(\"ChromaDB created and data saved\")\n",
        "        return vector_db\n",
        "\n",
        "    def create_tools(self):\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"retrieve_knowledge\",\n",
        "                description=\"Search mental health knowledge base for relevant information\",\n",
        "                func=self.retrieve_knowledge\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"analyze_emotion\",\n",
        "                description=\"Analyze emotional state and sentiment from user input\",\n",
        "                func=self.analyze_emotion\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"check_safety\",\n",
        "                description=\"Check for crisis indicators or safety concerns\",\n",
        "                func=self.check_safety\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def setup_qa_chain(self):\n",
        "        retriever = self.vector_db.as_retriever()\n",
        "        prompt_template = \"\"\"You are a compassionate mental health chatbot. Use the following context and emotional analysis to respond thoughtfully:\n",
        "\n",
        "Context: {context}\n",
        "Emotional Analysis: {emotion_analysis}\n",
        "Safety Check: {safety_status}\n",
        "\n",
        "User: {question}\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "        self.PROMPT = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=['context', 'emotion_analysis', 'safety_status', 'question']\n",
        "        )\n",
        "\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            chain_type_kwargs={\"prompt\": self.PROMPT}\n",
        "        )\n",
        "\n",
        "    def retrieve_knowledge(self, query: str) -> str:\n",
        "        try:\n",
        "            retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "            docs = retriever.get_relevant_documents(query)\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            return f\"Retrieved context: {context}\" if context else \"No relevant context found\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving knowledge: {str(e)}\"\n",
        "\n",
        "    def analyze_emotion(self, text: str) -> str:\n",
        "        try:\n",
        "            analysis = self.emotion_processor.process_text(text)\n",
        "            return f\"Emotional analysis: {analysis}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing emotion: {str(e)}\"\n",
        "\n",
        "    def check_safety(self, text: str) -> str:\n",
        "        crisis_keywords = ['suicide', 'kill myself', 'end it all', 'hurt myself', 'want to die']\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        crisis_detected = any(keyword in text_lower for keyword in crisis_keywords)\n",
        "\n",
        "        if crisis_detected:\n",
        "            return \"CRISIS DETECTED: Please seek immediate professional help. Contact a crisis hotline or emergency services.\"\n",
        "        return \"No immediate safety concerns detected\"\n",
        "\n",
        "    def chat(self, user_input: str) -> str:\n",
        "        try:\n",
        "            # Get context from knowledge base\n",
        "            context = self.retrieve_knowledge(user_input)\n",
        "\n",
        "            # Analyze emotions\n",
        "            emotion_analysis = self.analyze_emotion(user_input)\n",
        "\n",
        "            # Check safety\n",
        "            safety_status = self.check_safety(user_input)\n",
        "\n",
        "            # If crisis detected, return immediate response\n",
        "            if \"CRISIS DETECTED\" in safety_status:\n",
        "                return safety_status + \"\\n\\nI'm here to support you, but please reach out to a mental health professional or crisis hotline immediately. Your safety is the most important thing.\"\n",
        "\n",
        "            # Generate response using the QA chain\n",
        "            response = self.llm.invoke(f\"\"\"You are a compassionate mental health chatbot.\n",
        "\n",
        "Context from knowledge base: {context}\n",
        "Emotional analysis: {emotion_analysis}\n",
        "Safety status: {safety_status}\n",
        "\n",
        "User message: {user_input}\n",
        "\n",
        "Please provide a thoughtful, empathetic response that acknowledges the user's emotional state and provides helpful guidance.\"\"\")\n",
        "\n",
        "            return response.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"I'm sorry, I encountered an issue processing your message. Please try rephrasing your question. Error: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "def create_gradio_interface():\n",
        "    chatbot = MentalHealthMultiAgent()\n",
        "\n",
        "    def respond(user_message, chat_history):\n",
        "        bot_response = chatbot.chat(user_message)\n",
        "        return bot_response\n",
        "\n",
        "    demo = gr.ChatInterface(\n",
        "        fn=respond,\n",
        "        title=\"🧠 Multi-Agent Mental Health Chatbot\",\n",
        "        description=\"An advanced mental health chatbot with emotion analysis, safety monitoring, and knowledge retrieval capabilities.\",\n",
        "        examples=[\n",
        "            \"I'm feeling anxious about my upcoming exam\",\n",
        "            \"I've been feeling really sad lately\",\n",
        "            \"Can you help me with stress management techniques?\",\n",
        "            \"I'm having trouble sleeping\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Command Line Interface\n",
        "def main():\n",
        "    print(\"Initializing Multi-Agent Mental Health Chatbot...\")\n",
        "    chatbot = MentalHealthMultiAgent()\n",
        "\n",
        "    print(\"Multi-Agent Mental Health Chatbot initialized!\")\n",
        "    print(\"Type 'exit' to quit, 'gradio' to launch web interface\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Take care! Remember, professional help is always available.\")\n",
        "            break\n",
        "        elif user_input.lower() == 'gradio':\n",
        "            print(\"Launching Gradio interface...\")\n",
        "            demo = create_gradio_interface()\n",
        "            demo.launch()\n",
        "            break\n",
        "\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages\n",
        "    try:\n",
        "        import spacy\n",
        "        spacy.load(\"en_core_web_sm\")\n",
        "    except:\n",
        "        print(\"Installing required spaCy model...\")\n",
        "        os.system(\"pip install spacy textblob\")\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "LSXwscIvJf34",
        "outputId": "d6a6271e-93b3-4351-81b9-bc9f40c46ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Multi-Agent Mental Health Chatbot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "/tmp/ipython-input-40-664752842.py:103: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  self.conversation_memory = ConversationBufferMemory(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Agent Mental Health Chatbot initialized!\n",
            "Type 'exit' to quit, 'gradio' to launch web interface\n",
            "\n",
            "You: gradio\n",
            "Launching Gradio interface...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c24ad21eacfc26bb53.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c24ad21eacfc26bb53.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langgraph langchain-core langchain-community\n"
      ],
      "metadata": {
        "id": "rJawxWXlLvqe",
        "outputId": "ecca92a3-fcb5-49be-91bb-385aa8f061db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.66)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langgraph-checkpoint>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.5.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.1-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.5.0 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.1 langgraph-sdk-0.1.72 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crewai spacy textblob gradio langchain-groq chromadb sentence-transformers\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "EqP64WwfL52O",
        "outputId": "2806d263-2f67-4284-a2a2-f71397330ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai\n",
            "  Downloading crewai-0.134.0-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting appdirs>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python>=4.7.1 (from crewai)\n",
            "  Downloading auth0_python-4.10.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crewai) (8.2.1)\n",
            "Collecting instructor>=1.3.3 (from crewai)\n",
            "  Downloading instructor-1.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting json-repair>=0.25.2 (from crewai)\n",
            "  Downloading json_repair-0.47.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting json5>=0.10.0 (from crewai)\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting litellm==1.72.0 (from crewai)\n",
            "  Downloading litellm-1.72.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: onnxruntime==1.22.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.22.0)\n",
            "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.91.0)\n",
            "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from crewai) (3.1.5)\n",
            "Requirement already satisfied: opentelemetry-api>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.34.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.30.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.34.1)\n",
            "Collecting pdfplumber>=0.11.4 (from crewai)\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from crewai) (2.11.7)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.1.1)\n",
            "Collecting pyvis>=0.3.2 (from crewai)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from crewai) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from crewai) (0.21.2)\n",
            "Collecting tomli-w>=1.1.0 (from crewai)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tomli>=2.0.2 (from crewai)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting uv>=0.4.25 (from crewai)\n",
            "  Downloading uv-0.7.16-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm==1.72.0->crewai) (3.11.15)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.72.0->crewai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.72.0->crewai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm==1.72.0->crewai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.72.0->crewai) (4.24.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.72.0->crewai) (0.9.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (1.13.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.66)\n",
            "Requirement already satisfied: groq<1,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.29.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: cryptography>=43.0.1 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (43.0.3)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (2.10.1)\n",
            "Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (2.4.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.28.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm==1.72.0->crewai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm==1.72.0->crewai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.72.0->crewai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.16)\n",
            "Requirement already satisfied: jiter<0.11,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.10.0)\n",
            "Collecting mkdocs-material>=9.5.49 (from instructor>=1.3.3->crewai)\n",
            "  Downloading mkdocs_material-9.6.14-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting mkdocs>=1.6.1 (from instructor>=1.3.3->crewai)\n",
            "  Downloading mkdocs-1.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting pre-commit>=4.2.0 (from instructor>=1.3.3->crewai)\n",
            "  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (2.33.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.72.0->crewai) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.72.0->crewai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.72.0->crewai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.72.0->crewai) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-groq) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.30.0->crewai) (0.55b1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (3.4.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.4.1)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (4.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (3.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime==1.22.0->crewai) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.72.0->crewai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.72.0->crewai) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.72.0->crewai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.72.0->crewai) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.72.0->crewai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.72.0->crewai) (1.20.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=43.0.1->auth0-python>=4.7.1->crewai) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm==1.72.0->crewai) (3.23.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.66->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.66->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting ghp-import>=1.0 (from mkdocs>=1.6.1->instructor>=1.3.3->crewai)\n",
            "  Downloading ghp_import-2.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: markdown>=3.3.6 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (3.8.2)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs>=1.6.1->instructor>=1.3.3->crewai)\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mkdocs-get-deps>=0.2.0 (from mkdocs>=1.6.1->instructor>=1.3.3->crewai)\n",
            "  Downloading mkdocs_get_deps-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathspec>=0.11.1 (from mkdocs>=1.6.1->instructor>=1.3.3->crewai)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pyyaml-env-tag>=0.1 (from mkdocs>=1.6.1->instructor>=1.3.3->crewai)\n",
            "  Downloading pyyaml_env_tag-1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting watchdog>=2.0 (from mkdocs>=1.6.1->instructor>=1.3.3->crewai)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel~=2.10 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (2.17.0)\n",
            "Collecting backrefs~=5.7.post1 (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai)\n",
            "  Downloading backrefs-5.9-py311-none-any.whl.metadata (3.2 kB)\n",
            "Collecting colorama~=0.4 (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting mkdocs-material-extensions~=1.3 (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai)\n",
            "  Downloading mkdocs_material_extensions-1.3.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting paginate~=0.5 (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai)\n",
            "  Downloading paginate-0.5.7-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pymdown-extensions~=10.2 (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai)\n",
            "  Downloading pymdown_extensions-10.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit>=4.2.0->instructor>=1.3.3->crewai)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit>=4.2.0->instructor>=1.3.3->crewai)\n",
            "  Downloading identify-2.6.12-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit>=4.2.0->instructor>=1.3.3->crewai)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit>=4.2.0->instructor>=1.3.3->crewai)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime==1.22.0->crewai) (10.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from mkdocs-get-deps>=0.2.0->mkdocs>=1.6.1->instructor>=1.3.3->crewai) (4.3.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=4.2.0->instructor>=1.3.3->crewai)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Downloading crewai-0.134.0-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.1/327.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.72.0-py3-none-any.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.10.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.9.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.47.4-py3-none-any.whl (22 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading uv-0.7.16-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/18.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mkdocs-1.6.1-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mkdocs_material-9.6.14-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backrefs-5.9-py311-none-any.whl (392 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.1/392.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading identify-2.6.12-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Downloading mkdocs_get_deps-0.2.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading mkdocs_material_extensions-1.3.1-py3-none-any.whl (8.7 kB)\n",
            "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading paginate-0.5.7-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pymdown_extensions-10.16-py3-none-any.whl (266 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml_env_tag-1.1-py3-none-any.whl (4.7 kB)\n",
            "Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: paginate, distlib, appdirs, watchdog, virtualenv, uv, tomli-w, tomli, pyyaml-env-tag, pypdfium2, pymdown-extensions, pathspec, nodeenv, mkdocs-material-extensions, mergedeep, jsonref, json5, json-repair, jedi, identify, colorama, cfgv, backrefs, pre-commit, mkdocs-get-deps, ghp-import, pyvis, pdfminer.six, mkdocs, auth0-python, pdfplumber, mkdocs-material, litellm, opentelemetry-exporter-otlp-proto-http, instructor, crewai\n",
            "Successfully installed appdirs-1.4.4 auth0-python-4.10.0 backrefs-5.9 cfgv-3.4.0 colorama-0.4.6 crewai-0.134.0 distlib-0.3.9 ghp-import-2.1.0 identify-2.6.12 instructor-1.9.0 jedi-0.19.2 json-repair-0.47.4 json5-0.12.0 jsonref-1.1.0 litellm-1.72.0 mergedeep-1.3.4 mkdocs-1.6.1 mkdocs-get-deps-0.2.0 mkdocs-material-9.6.14 mkdocs-material-extensions-1.3.1 nodeenv-1.9.1 opentelemetry-exporter-otlp-proto-http-1.34.1 paginate-0.5.7 pathspec-0.12.1 pdfminer.six-20250506 pdfplumber-0.11.7 pre-commit-4.2.0 pymdown-extensions-10.16 pypdfium2-4.30.1 pyvis-0.3.2 pyyaml-env-tag-1.1 tomli-2.2.1 tomli-w-1.2.0 uv-0.7.16 virtualenv-20.31.2 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry"
                ]
              },
              "id": "2d5c64395f1846989fb9ac3e4d315a52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import gradio as gr\n",
        "\n",
        "# CrewAI imports\n",
        "from crewai import Agent, Task, Crew, Process, LLM\n",
        "from crewai.tools import BaseTool\n",
        "from typing import Type, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Global variables to store shared resources\n",
        "VECTOR_DB = None\n",
        "NLP_MODEL = None\n",
        "\n",
        "def initialize_nlp():\n",
        "    \"\"\"Initialize spaCy model globally\"\"\"\n",
        "    global NLP_MODEL\n",
        "    if NLP_MODEL is None:\n",
        "        try:\n",
        "            NLP_MODEL = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            os.system(\"python -m spacy download en_core_web_sm\")\n",
        "            NLP_MODEL = spacy.load(\"en_core_web_sm\")\n",
        "    return NLP_MODEL\n",
        "\n",
        "def load_existing_vector_db():\n",
        "    \"\"\"Load existing ChromaDB from persisted directory\"\"\"\n",
        "    global VECTOR_DB\n",
        "\n",
        "    db_path = \"./chroma_db\"\n",
        "\n",
        "    # Always try to load existing database first\n",
        "    if os.path.exists(db_path):\n",
        "        try:\n",
        "            embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "            VECTOR_DB = Chroma(\n",
        "                persist_directory=db_path,\n",
        "                embedding_function=embeddings\n",
        "            )\n",
        "\n",
        "            # Check if database has content\n",
        "            try:\n",
        "                collection_data = VECTOR_DB.get()\n",
        "                collection_count = len(collection_data['documents'])\n",
        "                print(f\"Loaded existing ChromaDB with {collection_count} documents\")\n",
        "\n",
        "                if collection_count == 0:\n",
        "                    print(\"Warning: ChromaDB exists but contains no documents\")\n",
        "                    print(\"Attempting to load documents from ./data/ folder...\")\n",
        "                    return create_vector_db_from_data()\n",
        "                else:\n",
        "                    return VECTOR_DB\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error checking database content: {str(e)}\")\n",
        "                return create_vector_db_from_data()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing ChromaDB: {str(e)}\")\n",
        "            print(\"Creating new vector database...\")\n",
        "            return create_vector_db_from_data()\n",
        "    else:\n",
        "        print(\"No existing ChromaDB found. Creating new one...\")\n",
        "        return create_vector_db_from_data()\n",
        "\n",
        "def create_vector_db_from_data():\n",
        "    \"\"\"Create new ChromaDB from PDF documents in data folder\"\"\"\n",
        "    global VECTOR_DB\n",
        "\n",
        "    # Ensure data directory exists\n",
        "    os.makedirs('./data/', exist_ok=True)\n",
        "\n",
        "\n",
        "    # Load documents from data folder\n",
        "    loader = DirectoryLoader(\"./data/\", glob='*.pdf', loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "\n",
        "    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "    if not documents:\n",
        "        print(\"No PDF documents found in ./data/ directory\")\n",
        "        print(\"Creating empty vector store...\")\n",
        "        VECTOR_DB = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)\n",
        "        return VECTOR_DB\n",
        "\n",
        "    print(f\"Found {len(documents)} PDF documents. Processing...\")\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"Split into {len(texts)} text chunks\")\n",
        "\n",
        "    # Create vector database\n",
        "    VECTOR_DB = Chroma.from_documents(\n",
        "        texts,\n",
        "        embeddings,\n",
        "        persist_directory='./chroma_db'\n",
        "    )\n",
        "\n",
        "    # Ensure persistence (for older ChromaDB versions)\n",
        "    try:\n",
        "        VECTOR_DB.persist()\n",
        "        print(\"Database persisted successfully\")\n",
        "    except AttributeError:\n",
        "        print(\"Auto-persistence enabled\")\n",
        "\n",
        "    print(f\"ChromaDB created with {len(texts)} document chunks and saved to ./chroma_db\")\n",
        "    return VECTOR_DB\n",
        "\n",
        "# Custom Tools for CrewAI (rest of your tool code remains the same)\n",
        "class KnowledgeRetrievalInput(BaseModel):\n",
        "    query: str = Field(description=\"The query to search for in the knowledge base\")\n",
        "\n",
        "class KnowledgeRetrievalTool(BaseTool):\n",
        "    name: str = \"knowledge_retrieval\"\n",
        "    description: str = \"Retrieves relevant mental health information from the knowledge base\"\n",
        "    args_schema: Type[BaseModel] = KnowledgeRetrievalInput\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        global VECTOR_DB\n",
        "        try:\n",
        "            if VECTOR_DB is None:\n",
        "                return \"Knowledge base not initialized\"\n",
        "\n",
        "            retriever = VECTOR_DB.as_retriever(search_kwargs={\"k\": 3})\n",
        "            docs = retriever.get_relevant_documents(query)\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            return f\"Retrieved context: {context}\" if context else \"No relevant context found\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving knowledge: {str(e)}\"\n",
        "\n",
        "# ... (rest of your EmotionAnalysisTool and SafetyMonitorTool code remains the same)\n",
        "\n",
        "class CrewAIMentalHealthChatbot:\n",
        "    def __init__(self):\n",
        "        self.setup_llm()\n",
        "        self.setup_vector_db()\n",
        "        self.setup_tools()\n",
        "        self.setup_agents()\n",
        "\n",
        "    def setup_llm(self):\n",
        "        self.llm = LLM(\n",
        "            model=\"groq/llama-3.3-70b-versatile\",\n",
        "            api_key=\"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\",\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "    def setup_vector_db(self):\n",
        "        global VECTOR_DB\n",
        "        # Load existing database or create new one from data\n",
        "        VECTOR_DB = load_existing_vector_db()\n",
        "\n",
        "        # Verify the database is working\n",
        "        if VECTOR_DB is not None:\n",
        "            try:\n",
        "                test_docs = VECTOR_DB.get()\n",
        "                doc_count = len(test_docs['documents'])\n",
        "                print(f\"Vector database ready with {doc_count} documents\")\n",
        "\n",
        "                if doc_count == 0:\n",
        "                    print(\"Note: Vector database is empty. Add PDF files to ./data/ folder and restart.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Vector database may not be functioning properly: {str(e)}\")\n",
        "\n",
        "    def create_vector_db(self):\n",
        "        os.makedirs('./data/', exist_ok=True)\n",
        "\n",
        "        loader = DirectoryLoader(\"./data/\", glob='*.pdf', loader_cls=PyPDFLoader)\n",
        "        documents = loader.load()\n",
        "\n",
        "        if not documents:\n",
        "            print(\"No PDF documents found in ./data/ directory\")\n",
        "            embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "            vector_db = Chroma(persist_directory='./chroma_db', embedding_function=embeddings)\n",
        "            return vector_db\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "        vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')\n",
        "        vector_db.persist()\n",
        "\n",
        "        print(\"ChromaDB created and data saved\")\n",
        "        return vector_db\n",
        "\n",
        "    def setup_tools(self):\n",
        "        # Initialize spaCy model before creating tools\n",
        "        initialize_nlp()\n",
        "\n",
        "        self.knowledge_tool = KnowledgeRetrievalTool()\n",
        "        self.emotion_tool = EmotionAnalysisTool()\n",
        "        self.safety_tool = SafetyMonitorTool()\n",
        "\n",
        "    def setup_agents(self):\n",
        "        # Emotional Intelligence Agent\n",
        "        self.emotion_agent = Agent(\n",
        "            role='Emotional Intelligence Specialist',\n",
        "            goal='Analyze user emotions and psychological state accurately',\n",
        "            backstory=\"\"\"You are an expert in emotional intelligence and psychological assessment.\n",
        "            Your role is to carefully analyze user messages for emotional content, sentiment,\n",
        "            and psychological indicators. You provide detailed emotional assessments that help\n",
        "            other agents understand the user's mental state.\"\"\",\n",
        "            tools=[self.emotion_tool],\n",
        "            llm=self.llm,\n",
        "            verbose=True,\n",
        "            allow_delegation=False\n",
        "        )\n",
        "\n",
        "        # Knowledge Retrieval Agent\n",
        "        self.knowledge_agent = Agent(\n",
        "            role='Mental Health Knowledge Specialist',\n",
        "            goal='Retrieve and synthesize relevant mental health information',\n",
        "            backstory=\"\"\"You are a mental health knowledge specialist with access to\n",
        "            comprehensive mental health resources. Your role is to find and synthesize\n",
        "            relevant information from the knowledge base to support therapeutic conversations.\"\"\",\n",
        "            tools=[self.knowledge_tool],\n",
        "            llm=self.llm,\n",
        "            verbose=True,\n",
        "            allow_delegation=False\n",
        "        )\n",
        "\n",
        "        # Safety Monitor Agent\n",
        "        self.safety_agent = Agent(\n",
        "            role='Crisis Intervention Specialist',\n",
        "            goal='Monitor for safety risks and crisis situations',\n",
        "            backstory=\"\"\"You are a crisis intervention specialist responsible for identifying\n",
        "            immediate safety risks and crisis situations. Your primary concern is user safety,\n",
        "            and you must flag any concerning content that requires immediate attention.\"\"\",\n",
        "            tools=[self.safety_tool],\n",
        "            llm=self.llm,\n",
        "            verbose=True,\n",
        "            allow_delegation=False\n",
        "        )\n",
        "\n",
        "        # Therapeutic Response Agent\n",
        "        self.therapist_agent = Agent(\n",
        "            role='Compassionate Mental Health Therapist',\n",
        "            goal='Provide empathetic, therapeutic responses based on analysis from other agents',\n",
        "            backstory=\"\"\"You are a compassionate mental health therapist with expertise in\n",
        "            various therapeutic approaches. You synthesize information from emotional analysis,\n",
        "            knowledge retrieval, and safety assessments to provide thoughtful, empathetic,\n",
        "            and therapeutically appropriate responses to users.\"\"\",\n",
        "            tools=[],\n",
        "            llm=self.llm,\n",
        "            verbose=True,\n",
        "            allow_delegation=False\n",
        "        )\n",
        "\n",
        "    def chat(self, user_input: str) -> str:\n",
        "        try:\n",
        "            # Define tasks for each agent\n",
        "            emotion_task = Task(\n",
        "                description=f\"Use the emotion_analysis tool to analyze this message: '{user_input}'. Provide detailed emotional insights.\",\n",
        "                agent=self.emotion_agent,\n",
        "                expected_output=\"Detailed emotional analysis including sentiment, detected emotions, and intensity level\"\n",
        "            )\n",
        "\n",
        "            knowledge_task = Task(\n",
        "                description=f\"Use the knowledge_retrieval tool to find relevant mental health information for: '{user_input}'. Focus on therapeutic guidance.\",\n",
        "                agent=self.knowledge_agent,\n",
        "                expected_output=\"Relevant mental health information and therapeutic guidance from knowledge base\"\n",
        "            )\n",
        "\n",
        "            safety_task = Task(\n",
        "                description=f\"Use the safety_monitor tool to assess this message for crisis indicators: '{user_input}'. Report any safety concerns.\",\n",
        "                agent=self.safety_agent,\n",
        "                expected_output=\"Safety assessment with risk level and any crisis alerts\"\n",
        "            )\n",
        "\n",
        "            therapy_task = Task(\n",
        "                description=f\"\"\"Based on the previous analyses, provide a compassionate therapeutic response to: '{user_input}'.\n",
        "\n",
        "                Consider the emotional state, relevant knowledge, and safety concerns to craft an appropriate response.\n",
        "                Be warm, empathetic, and provide helpful guidance while validating the user's feelings.\"\"\",\n",
        "                agent=self.therapist_agent,\n",
        "                expected_output=\"Compassionate therapeutic response addressing the user's needs\",\n",
        "                context=[emotion_task, knowledge_task, safety_task]\n",
        "            )\n",
        "\n",
        "            # Create crew with tasks\n",
        "            task_crew = Crew(\n",
        "                agents=[self.emotion_agent, self.knowledge_agent, self.safety_agent, self.therapist_agent],\n",
        "                tasks=[emotion_task, knowledge_task, safety_task, therapy_task],\n",
        "                process=Process.sequential,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Execute the crew workflow\n",
        "            result = task_crew.kickoff()\n",
        "\n",
        "            # Extract the final therapeutic response\n",
        "            if hasattr(result, 'raw'):\n",
        "                return result.raw\n",
        "            else:\n",
        "                return str(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"I apologize, but I encountered an issue processing your message. Please try again. Error: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "def create_gradio_interface():\n",
        "    print(\"Initializing CrewAI Mental Health Chatbot...\")\n",
        "    chatbot = CrewAIMentalHealthChatbot()\n",
        "\n",
        "    def respond(user_message, chat_history):\n",
        "        bot_response = chatbot.chat(user_message)\n",
        "        return bot_response\n",
        "\n",
        "    demo = gr.ChatInterface(\n",
        "        fn=respond,\n",
        "        title=\"🧠 CrewAI Multi-Agent Mental Health Chatbot\",\n",
        "        description=\"\"\"\n",
        "        An advanced mental health chatbot powered by CrewAI multi-agent workflow:\n",
        "        • **Emotion Agent**: Analyzes your emotional state\n",
        "        • **Knowledge Agent**: Retrieves relevant mental health information\n",
        "        • **Safety Agent**: Monitors for crisis situations\n",
        "        • **Therapist Agent**: Provides compassionate therapeutic responses\n",
        "        \"\"\",\n",
        "        examples=[\n",
        "            \"I'm feeling overwhelmed with work stress\",\n",
        "            \"I've been having trouble sleeping and feeling anxious\",\n",
        "            \"Can you help me understand depression better?\",\n",
        "            \"I'm going through a difficult breakup\"\n",
        "        ],\n",
        "        theme=\"soft\"\n",
        "    )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Command Line Interface\n",
        "def main():\n",
        "    print(\"Initializing CrewAI Multi-Agent Mental Health Chatbot...\")\n",
        "    chatbot = CrewAIMentalHealthChatbot()\n",
        "\n",
        "    print(\"\\n🧠 CrewAI Mental Health Chatbot Ready!\")\n",
        "    print(\"Type 'exit' to quit, 'gradio' to launch web interface\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Take care! Remember, professional help is always available if you need it.\")\n",
        "            break\n",
        "        elif user_input.lower() == 'gradio':\n",
        "            print(\"Launching Gradio interface...\")\n",
        "            demo = create_gradio_interface()\n",
        "            demo.launch()\n",
        "            break\n",
        "\n",
        "        print(\"🤖 Processing with multi-agent workflow...\")\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set environment variable for Groq API key\n",
        "    os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "    # Install required packages\n",
        "    print(\"Installing required packages...\")\n",
        "    os.system(\"pip install crewai spacy textblob gradio langchain-groq chromadb sentence-transformers\")\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "NErvb2rbLy3Q",
        "outputId": "2b5c9d84-42af-4af6-ef1c-7dec4f36bd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "Initializing CrewAI Multi-Agent Mental Health Chatbot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing ChromaDB with 0 documents\n",
            "Warning: ChromaDB exists but contains no documents\n",
            "Attempting to load documents from ./data/ folder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13 PDF documents. Processing...\n",
            "Split into 96 text chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-51-547737458.py:108: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  VECTOR_DB.persist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database persisted successfully\n",
            "ChromaDB created with 96 document chunks and saved to ./chroma_db\n",
            "Vector database ready with 96 documents\n",
            "\n",
            "🧠 CrewAI Mental Health Chatbot Ready!\n",
            "Type 'exit' to quit, 'gradio' to launch web interface\n",
            "\n",
            "You: gradio\n",
            "Launching Gradio interface...\n",
            "Initializing CrewAI Mental Health Chatbot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing ChromaDB with 96 documents\n",
            "Vector database ready with 96 documents\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a57eb3a52a3254c7e9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a57eb3a52a3254c7e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}